{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Injection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='In the realm of data, machines learn and grow, Through algorithms and patterns they show. They sift through information, finding trends, Predicting outcomes, solving complex bends. With each iteration, they refine their code, Unraveling mysteries, unlocking the code. From self-driving cars to medical cures, Machine learning technology ensures A brighter future, where possibilities soar, Guided by data, like never before.', metadata={'source': 'speech.txt'})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "text_loader = TextLoader('speech.txt')\n",
    "text_loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Content Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv() \n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n      How to Build an Open-Domain Question Answering System?\\n    \\nDate: October 29, 2020  |  Estimated Reading Time: 33 min  |  Author: Lilian Weng\\n\\n\\n\\n[Updated on 2020-11-12: add an example on closed-book factual QA using OpenAI API (beta).\\nA model that can answer any question with regard to factual knowledge can lead to many useful and practical applications, such as working as a chatbot or an AI assistantü§ñ. In this post, we will review several common approaches for building such an open-domain question answering system.\\nDisclaimers given so many papers in the wild:\\n\\nAssume we have access to a powerful pretrained language model.\\nWe do not cover how to use structured knowledge base (e.g. Freebase, WikiData) here.\\nWe only focus on a single-turn QA instead of a multi-turn conversation style QA.\\nWe mostly focus on QA models that contain neural networks, specially Transformer-based language models.\\nI admit that I missed a lot of papers with architectures designed specifically for QA tasks between 2017-2019üòî\\n\\nWhat is Open-Domain Question Answering?#\\nOpen-domain Question Answering (ODQA) is a type of language tasks, asking a model to produce answers to factoid questions in natural language. The true answer is objective, so it is simple to evaluate model performance.\\nFor example,\\nQuestion: What did Albert Einstein win the Nobel Prize for?\\nAnswer: The law of the photoelectric effect.\\nThe ‚Äúopen-domain‚Äù part refers to the lack of the relevant context for any arbitrarily asked factual question. In the above case, the model only takes as the input the question but no article about ‚Äúwhy Einstein didn‚Äôt win a Nobel Prize for the theory of relativity‚Äù is provided, where the term ‚Äúthe law of the photoelectric effect‚Äù is likely mentioned. In the case when both the question and the context are provided, the task is known as Reading comprehension (RC).\\nAn ODQA model may work with or without access to an external source of knowledge (e.g. Wikipedia) and these two conditions are referred to as open-book or closed-book question answering, respectively.\\nWhen considering different types of open-domain questions, I like the classification by Lewis, et al., 2020, in increasing order of difficulty:\\n\\nA model is able to correctly memorize and respond with the answer to a question that has been seen at training time.\\nA model is able to answer novel questions at test time and choose an answer from the set of answers it has seen during training.\\nA model is able to answer novel questions which have answers not contained in the training dataset.\\n\\n\\nFig. 1. Overview of three frameworks discussed in this post.\\nNotation#\\nGiven a question $x$ and a ground truth answer span $y$, the context passage containing the true answer is labelled as $z \\\\in \\\\mathcal{Z}$, where $\\\\mathcal{Z}$ is an external knowledge corpus. Wikipedia is a common choice for such an external knowledge source.\\nConcerns of QA data fine-tuning#\\nBefore we dive into the details of many models below. I would like to point out one concern of fine-tuning a model with common QA datasets, which appears as one fine-tuning step in several ODQA models. It could be concerning, because there is a significant overlap between questions in the train and test sets in several public QA datasets.\\nLewis, et al., (2020) (code) found that 58-71% of test-time answers are also present somewhere in the training sets and 28-34% of test-set questions have a near-duplicate paraphrase in their corresponding training sets. In their experiments, several models performed notably worse when duplicated or paraphrased questions were removed from the training set.\\nOpen-book QA: Retriever-Reader#\\nGiven a factoid question, if a language model has no context or is not big enough to memorize the context which exists in the training dataset, it is unlikely to guess the correct answer. In an open-book exam, students are allowed to refer to external resources like notes and books while answering test questions. Similarly, a ODQA system can be paired with a rich knowledge base to identify relevant documents as evidence of answers.\\nWe can decompose the process of finding answers to given questions into two stages,\\n\\nFind the related context in an external repository of knowledge;\\nProcess the retrieved context to extract an answer.\\n\\n\\nFig. 2. The retriever-reader QA framework combines information retrieval with machine reading comprehension.\\nSuch a retriever + reader framework was first proposed in DrQA (‚ÄúDocument retriever Question-Answering‚Äù by Chen et al., 2017; code). The retriever and the reader components can be set up and trained independently, or jointly trained end-to-end.\\nRetriever Model#\\nTwo popular approaches for implementing the retriever is to use the information retrieval (IR) system that depends on (1) the classic non-learning-based TF-IDF features (‚Äúclassic IR‚Äù) or (2) dense embedding vectors of text produced by neural networks (‚Äúneural IR‚Äù).\\nClassic IR#\\nDrQA (Chen et al., 2017) adopts an efficient non-learning-based search engine based on the vector space model. Every query and document is modelled as a bag-of-word vector, where each term is weighted by TF-IDF (term frequency $\\\\times$ inverse document frequency).\\n\\n$$\\n\\\\begin{aligned}\\n\\\\text{tf-idf}(t, d, \\\\mathcal{D}) &= \\\\text{tf}(t, d) \\\\times \\\\text{idf}(t, \\\\mathcal{D}) \\\\\\\\\\n\\\\text{tf}(t, d) &= \\\\log(1 + \\\\text{freq}(t, d)) \\\\\\\\\\n\\\\text{idf}(t, \\\\mathcal{D}) &= \\\\log \\\\Big( \\\\frac{\\\\vert\\\\mathcal{D}\\\\vert}{\\\\vert d\\\\in\\\\mathcal{D}: t\\\\in d\\\\vert} \\\\Big)\\n\\\\end{aligned}\\n$$\\n\\nwhere $t$ is a unigram or bigram term in a document $d$ from a collection of documents $\\\\mathcal{D}$ . $\\\\text{freq}(t, d)$ measures how many times a term $t$ appears in $d$. Note that the term-frequency here includes bigram counts too, which is found to be very helpful because the local word order is taken into consideration via bigrams. As part of the implementation, DrQA maps the bigrams of $2^{24}$ bins using unsigned murmur3 hash.\\nPrecisely, DrQA implemented Wikipedia as its knowledge source and this choice has became a default setting for many ODQA studies since then. The non-ML document retriever returns the top $k=5$ most relevant Wikipedia articles given a question.\\nBERTserini (Yang et al., 2019) pairs the open-source Anserini IR toolkit as the retriever with a fine-tuned pre-trained BERT model as the reader. The top $k$ documents ($k=10$) are retrieved via the post-v3.0 branch of Anserini with the query treated as a bag of words. The retrieved text segments are ranked by BM25, a classic TF-IDF-based retrieval scoring function. In terms of the effect of text granularity on performance, they found that paragraph retrieval > sentence retrieval > article retrieval.\\n\\nFig. 3. An illustration of BERTserini architecture. (Image source: Yang et al., 2019)\\nElasticSearch + BM25 is used by the Multi-passage BERT QA model (Wang et al., 2019). They found that splitting articles into passages with the length of 100 words by sliding window brings 4% improvements, since splitting documents into passages without overlap may cause some near-boundary evidence to lose useful contexts.\\nNeural IR#\\nThere is a long history in learning a low-dimensional representation of text, denser than raw term-based vectors (Deerwester et al., 1990; Yih, et al., 2011). Dense representations can be learned through matrix decomposition or some neural network architectures (e.g. MLP, LSTM, bidirectional LSTM, etc). When involving neural networks, such approaches are referred to as ‚ÄúNeural IR‚Äù, Neural IR is a new category of methods for retrieval problems, but it is not necessary to perform better/superior than classic IR (Lim, 2018).\\nAfter the success of many large-scale general language models, many QA models embrace the following approach:\\n\\n$$\\nh_x = E_x(x)\\\\quad\\nh_z = E_z(z)\\\\quad\\n\\\\text{score}(x, z) = h_x^\\\\top h_z\\n$$\\n\\n\\nExtract the dense representations of a question $x$ and a context passage $z$ by feeding them into a language model;\\nUse the dot-product of these two representations as the retrieval score to rank and select most relevant passages.\\n\\nORQA, REALM and DPR all use such a scoring function for context retrieval, which will be described in detail in a later section on the end-to-end QA model.\\nAn extreme approach, investigated by DenSPI (‚ÄúDense-Sparse Phrase Index‚Äù; Seo et al., 2019), is to encode all the text in the knowledge corpus at the phrase level and then only rely on the retriever to identify the most relevant phrase as the predicted answer. In this way, the retriever+reader pipeline is reduced to only retriever. Of course, the index would be much larger and the retrieval problem is more challenging.\\nDenSPI introduces a query-agnostic indexable representation of document phrases. Precisely it encodes query-agnostic representations of text spans in Wikipedia offline and looks for the answer at inference time by performing nearest neighbor search. It can drastically speed up the inference time, because there is no need to re-encode documents for every new query, which is often required by a reader model.\\nGiven a question $x$ and a fixed set of (Wikipedia) documents, $z_1, \\\\dots, z_K$ and each document $z_k$ contains $N_k$ words, $z_k = \\\\langle z_k^{(1)}, \\\\dots, z_k^{(N_k)}\\\\rangle$. An ODQA model is a scoring function $F$ for each candidate phrase span $z_k^{(i:j)}, 1 \\\\leq i \\\\leq j \\\\leq N_k$, such that the truth answer is the phrase with maximum score: $y = {\\\\arg\\\\max}_{k,i,j} F(x, z_k^{(i:j)})$.\\nThe phrase representation $z_k^{(i:j)}$ combines both dense and sparse vectors, $z_k^{(i:j)} = [d_k^{(i:j)}, s_k^{(i:j)}] \\\\in \\\\mathbb{R}^{d^d + d^s}$ (note that $d^d \\\\ll d^s$):\\n\\nThe dense vector $d_k^{(i:j)}$ is effective for encoding local syntactic and semantic cues, as what can be learned by a pretrained language model.\\nThe sparse vector $s_k^{(i:j)}$ is superior at encoding precise lexical information. The sparse vector is term-frequency-based encoding. DenSPI uses 2-gram term-frequency same as DrQA, resulting a highly sparse representation ($d^s \\\\approx 16$M)\\n\\nThe dense vector $d^{(i:j)}$ is further decomposed into three parts, $d^{(i:j)} = [a_i, b_j, c_{ij}] \\\\in \\\\mathbb{R}^{2d^b + 1}$ where $2d^b + 1 = d^d$. All three components are learned based on different columns of the fine-tuned BERT representations.\\n\\nA vector $a_i$ encodes the start position for the $i$-th word of the document;\\nA vector $b_j$ encodes the end position for the $j$-th word of the document;\\nA scalar $c_{ij}$ measures the coherency between the start and the end vectors, helping avoid non-constituent phrases during inference.\\n\\nFor all possible $(i,j,k)$ tuples where $j-i < J$, the text span embeddings are precomputed and stored as a phrase index. The maximum span length $J$ is a predefined scalar constant.\\n\\nFig. 4. An illustration of Dense-Sparse Phrase Index (DenSPI) architecture. (Image source: Seo et al., 2019)\\nAt the inference time, the question is mapped into the same vector space $x=[d‚Äô, s‚Äô] \\\\in \\\\mathbb{R}^{d^d + d^s}$, where the dense vector $d‚Äô$ is extracted from the BERT embedding of the special [CLS] symbol. The same BERT model is shared for encoding both questions and phrases. The final answer is predicted by $k^*, i^*, j^* = \\\\arg\\\\max x^\\\\top z_k^{(i:j)}$.\\nReader Model#\\nThe reader model learns to solve the reading comprehension task ‚Äî extract an answer for a given question from a given context document. Here we only discuss approaches for machine comprehension using neural networks.\\nBi-directional LSTM#\\nThe reader model for answer detection of DrQA (Chen et al., 2017) is a 3-layer bidirectional LSTM with hidden size 128. Every relevant paragraph of retrieved Wikipedia articles is encoded by a sequence of feature vector, $\\\\{\\\\tilde{\\\\mathbf{z}}_1, \\\\dots, \\\\tilde{\\\\mathbf{z}}_m \\\\}$. Each feature vector $\\\\hat{\\\\mathbf{z}}_i \\\\in \\\\mathbb{R}^{d_z}$ is expected to capture useful contextual information around one token $z_i$. The feature consists of several categories of features:\\n\\nWord embeddings: A 300d Glove word embedding trained from 800B Web crawl data, $f_\\\\text{embed} = E_g(z_i)$.\\nExact match: Whether a word $z_i$ appears in the question $x$, $f_\\\\text{match} = \\\\mathbb{I}(z_i \\\\in x)$.\\nToken features: This includes POS (part-of-speech) tagging, NER (named entity recognition), and TF (term-frequency), $f_\\\\text{token}(z_i) = (\\\\text{POS}(z_i), \\\\text{NER}(z_i), \\\\text{TF}(z_i))$.\\nAligned question embedding: The attention score $y_{ij}$ is designed to capture inter-sentence matching and similarity between the paragraph token $z_i$ and the question word $x_j$. This feature adds soft alignments between similar but non-identical words.\\n\\n\\n$$\\n\\\\begin{aligned}\\nf_\\\\text{align}(z_i) &= \\\\sum_j y_{i,j} E_g(x_j) \\\\\\\\ \\ny_{i,j} &= \\\\frac{\\\\exp(\\\\alpha(E_g(z_i))^\\\\top \\\\alpha(E_g(x_j)) )}{\\\\sum_{j\\'} \\\\exp(\\\\alpha(E_g(z_i))^\\\\top \\\\alpha(E_g(x_{j\\'})) ) }\\n\\\\end{aligned}\\n$$\\n\\nwhere $\\\\alpha$ is a single dense layer with ReLU and $E_g(.)$ is the glove word embedding.\\nThe feature vector of a paragraph of $m$ tokens is fed into LSTM to obtain the final paragraph vectors:\\n\\n$$\\n\\\\begin{aligned}\\n\\\\mathbf{z} = \\\\{\\\\mathbf{z}_1, \\\\dots, \\\\mathbf{z}_m\\\\} &= \\\\text{LSTM}(\\\\{\\\\tilde{\\\\mathbf{z}}_1, \\\\dots, \\\\tilde{\\\\mathbf{z}}_m\\\\}) \\\\\\\\\\n\\\\text{where } \\\\tilde{\\\\mathbf{z}}_i &= \\\\{f_\\\\text{embed}, f_\\\\text{match}, f_\\\\text{token}, f_\\\\text{align}\\\\}\\n\\\\end{aligned}\\n$$\\n\\nThe question is encoded as a weighted sum of the embeddings of every word in the question:\\n\\n$$\\n\\\\mathbf{x} = \\\\sum_j b_j E(x_j) \\\\quad b_j = \\\\text{softmax}(\\\\mathbf{w}^\\\\top E(x_j))\\n$$\\n\\nwhere $\\\\mathbf{w}$ is a weight vector to learn.\\nOnce the feature vectors are constructed for the question and all the related paragraphs, the reader needs to predict the probabilities of each position in a paragraph to be the start and the end of an answer span, $p_\\\\text{start}(i_s)$ and $p_\\\\text{end}(i_s)$, respectively. Across all the paragraphs, the optimal span is returned as the final answer with maximum $p_\\\\text{start}(i_s)  \\\\times p_\\\\text{end}(i_e) $.\\n\\n$$\\n\\\\begin{aligned}\\np_\\\\text{start}(i_s) \\\\propto \\\\exp(\\\\mathbf{z}_{i_s} \\\\mathbf{W}_s \\\\mathbf{x}) \\\\\\\\ \\np_\\\\text{end}(i_e) \\\\propto \\\\exp(\\\\mathbf{z}_{i_e} \\\\mathbf{W}_e \\\\mathbf{x}) \\\\\\\\\\n\\\\text{ s.t. } i_s \\\\leq i_e \\\\leq i_s + 15\\n\\\\end{aligned}\\n$$\\n\\nwhere $\\\\mathbf{W}_s$ and $\\\\mathbf{W}_e$ are learned parameters.\\nBERT-universe#\\nFollowing the success of BERT (Devlin et al., 2018), many QA models develop the machine comprehension component based on BERT. Let‚Äôs define the BERT model as a function that can take one or multiple strings (concatenated by [SEP]) as input and outputs a set of BERT encoding vectors for the special [CLS] token and every input token:\\n\\n$$\\n\\\\text{BERT}(s_1, s_2, \\\\dots) = [\\\\mathbf{h}^\\\\texttt{[CLS]}, \\\\mathbf{h}^{(1)}, \\\\mathbf{h}^{(2)}, \\\\dots]\\n$$\\n\\nwhere $\\\\mathbf{h}^\\\\texttt{[CLS]}$ is the embedding vector for the special [CLS] token and $\\\\mathbf{h}^{(i)}$ is the embedding vector for the $i$-th token.\\nTo use BERT for reading comprehension, it learns two additional weights, $\\\\mathbf{W}_s$ and $\\\\mathbf{W}_e$, and $\\\\text{softmax}(\\\\mathbf{h}^{(i)}\\\\mathbf{W}_s)$ and $\\\\text{softmax}(\\\\mathbf{h}^{(i)}\\\\mathbf{W}_e)$ define two probability distributions of start and end position of the predicted span per token.\\nBERTserini (Yang et al., 2019) utilizes a pre-trained BERT model to work as the reader. Their experiments showed that fine-tuning pretrained BERT with SQuAD is sufficient to achieve high accuracy in identifying answer spans.\\n\\nFig. 5. How BERT is used to solve question-answering tasks. (Image source: Devlin et al., 2018)\\nThe key difference of the BERTserini reader from the original BERT is: to allow comparison and aggregation of results from different segments, the final softmax layer over different answer spans is removed. The pre-trained BERT model is fine-tuned on the training set of SQuAD, where all inputs to the reader are padded to 384 tokens with the learning rate 3e-5.\\nWhen ranking all the extracted answer spans, the retriever score (BM25) and the reader score (probability of token being the start position $\\\\times$ probability of the same token being the end position ) are combined via linear interpolation.\\nThe original BERT normalizes the probability distributions of start and end position per token for every passage independently. Differently, the Multi-passage BERT (Wang et al., 2019) normalizes answer scores across all the retrieved passages of one question globally. Precisely, multi-passage BERT removes the final normalization layer per passage in BERT for QA (same as in BERTserini) and then adds a global softmax over all the word positions of all the passages. Global normalization makes the reader model more stable while pin-pointing answers from a large number of passages.\\nIn addition, multi-passage BERT implemented an independent passage ranker model via another BERT model and the rank score for $(x, z)$ is generated by a softmax over the representation vectors of the first [CLS] token. The passage ranker brings in extra 2% improvements. Similar idea of re-ranking passages with BERT was discussed in Nogueira & Cho, 2019, too.\\nInterestingly, Wang et al., 2019 found that explicit inter-sentence matching does not seem to be critical for RC tasks with BERT; check the original paper for how the experiments were designed. One possible reason is that the multi-head self-attention layers in BERT has already embedded the inter-sentence matching.\\nEnd-to-end Joint Training#\\nThe retriever and reader components can be jointly trained. This section covers R^3, ORQA, REALM and DPR. There are a lot of common designs, such as BERT-based dense vectors for retrieval and the loss function on maximizing the marginal likelihood of obtaining true answers.\\nThe retriever and reader models in the R^3 (‚ÄúReinforced Ranker-Reader‚Äù; Wang, et al., 2017) QA system are jointly trained via reinforcement learning. (Note that to keep the term consistent between papers in this section, the ‚Äúranker‚Äù model in the original R^3 paper is referred to as the ‚Äúretriever‚Äù model here.) Both components are variants of Match-LSTM, which relies on an attention mechanism to compute word similarities between the passage and question sequences.\\nHow does the Match-LSTM module work? Given a question $\\\\mathbf{X}$ of $d_x$ words and a passage $\\\\mathbf{Z}$ of $d_z$ words, both representations use fixed Glove word embeddings,\\n\\n$$\\n\\\\begin{aligned}\\n\\\\mathbf{H}^x &= \\\\text{BiLSTM}(\\\\mathbf{X}) \\\\in \\\\mathbb{R}^{l \\\\times d_x} \\\\\\\\\\n\\\\mathbf{H}^z &= \\\\text{BiLSTM}(\\\\mathbf{Z}) \\\\in \\\\mathbb{R}^{l \\\\times d_z} \\\\\\\\\\n\\\\mathbf{G} &= \\\\text{softmax}((\\\\mathbf{W}^g \\\\mathbf{H}^x + \\\\mathbf{b}^g \\\\otimes \\\\mathbf{e}_{d_x})^\\\\top \\\\mathbf{H}^z) \\\\in \\\\mathbb{R}^{d_x \\\\times d_z} & \\\\text{; an attention matrix}\\\\\\\\\\n\\\\bar{\\\\mathbf{H}}^x &= \\\\mathbf{H}^x \\\\mathbf{G} \\\\in \\\\mathbb{R}^{l \\\\times d_z} \\\\\\\\\\n\\\\mathbf{M} &= \\\\text{ReLU} \\\\Big( \\\\mathbf{W}^m \\\\begin{bmatrix}\\n\\\\mathbf{H}^z \\\\\\\\\\n\\\\bar{\\\\mathbf{H}}^x \\\\\\\\\\n\\\\mathbf{H}^z \\\\odot \\\\bar{\\\\mathbf{H}}^x \\\\\\\\\\n\\\\mathbf{H}^z - \\\\bar{\\\\mathbf{H}}^x\\n\\\\end{bmatrix} \\\\Big) \\\\in \\\\mathbb{R}^{2l \\\\times d_z} \\\\\\\\\\n\\\\mathbf{H}^m &= \\\\text{BiLSTM}(M) \\\\in \\\\mathbb{R}^{l \\\\times d_z}\\n\\\\end{aligned}\\n$$\\n\\nwhere $l$ is the hidden dimension of the bidirectional LSTM module. $\\\\mathbf{W}^g \\\\in \\\\mathbb{R}^{l\\\\times l}$, $\\\\mathbf{b}^g \\\\in \\\\mathbb{R}^l$, and $\\\\mathbf{W}^m \\\\in \\\\mathbb{R}^{2l \\\\times 4l}$ are parameters to learn. The operator $\\\\otimes \\\\mathbf{e}_{d_x}$ is the outer product to repeat the column vector $\\\\mathbf{b}^g$ $d_x$ times.\\nThe ranker and reader components share the same Match-LSTM module with two separate prediction heads in the last layer, resulting in $\\\\mathbf{H}^\\\\text{rank}$ and $\\\\mathbf{H}^\\\\text{reader}$.\\n\\nFig. 6. The overview of R^3 (reinforced ranker-reader) architecture. Both components share the same Match-LSTM module. (Image source: Wang, et al., 2017)\\nThe retriever runs a max-pooling operation per passage and then aggregates to output a probability of each passage entailing the answer.\\n\\n$$\\n\\\\begin{aligned}\\n\\\\mathbf{u}_i &= \\\\text{max-pooling}(\\\\mathbf{H}^\\\\text{rank}_i) \\\\in \\\\mathbb{R}^l \\\\\\\\\\n\\\\mathbf{C} &= \\\\text{tanh}(\\\\mathbf{W}^c[\\\\mathbf{u}_1;\\\\dots;\\\\mathbf{u}_N] + \\\\mathbf{b}^c \\\\otimes \\\\mathbf{e}_N) \\\\in \\\\mathbb{R}^{l \\\\times n} \\\\\\\\\\n\\\\gamma &= \\\\text{softmax}(\\\\mathbf{w}^c \\\\mathbf{C}) \\\\in \\\\mathbb{R}^n\\n\\\\end{aligned}\\n$$\\n\\nFinally, the retriever is viewed as a policy to output action to sample a passage according to predicted $\\\\gamma$,\\n\\n$$\\n\\\\pi(z \\\\vert x; \\\\theta^\\\\gamma) = \\\\gamma_z\\n$$\\n\\nThe reader predicts the start position $\\\\beta^s$ and the end position $\\\\beta^e$ of the answer span. Two positions are computed in the same way, with independent parameters to learn. There are $V$ words in all the passages involved.\\n\\n$$\\n\\\\begin{aligned}\\n\\\\mathbf{H}^\\\\text{read} &= [\\\\mathbf{H}^\\\\text{read}_\\\\tau; \\\\mathbf{H}^\\\\text{read}_{\\\\text{neg}_1}; \\\\dots; \\\\mathbf{H}^\\\\text{read}_{\\\\text{neg}_n}] \\\\\\\\\\n\\\\mathbf{F}^s &= \\\\text{tanh}(\\\\mathbf{W}^s \\\\mathbf{H}^\\\\text{read} + \\\\mathbf{b}^s \\\\otimes \\\\mathbf{e}_V) \\\\quad\\n\\\\beta^s = \\\\text{softmax}(\\\\mathbf{w}^s \\\\mathbf{F}^s) \\\\in \\\\mathbb{R}^V \\\\\\\\\\n\\\\mathbf{F}^e &= \\\\text{tanh}(\\\\mathbf{W}^e \\\\mathbf{H}^\\\\text{read} + \\\\mathbf{b}^e \\\\otimes \\\\mathbf{e}_V) \\\\quad\\n\\\\beta^e = \\\\text{softmax}(\\\\mathbf{w}^e \\\\mathbf{F}^e) \\\\in \\\\mathbb{R}^V \\\\\\\\\\nL(y \\\\vert z, x) &= -\\\\log(\\\\beta^s_{y_z^s})-\\\\log(\\\\beta^e_{y_z^e})\\n\\\\end{aligned}\\n$$\\n\\nwhere $y$ is the ground-truth answer and the passage $z$ is sampled by the retriever. $\\\\beta^s_{y_z^s}$ and $\\\\beta^s_{y_z^e}$ represent the probabilities of the start and end positions of $y$ in passage $z$.\\nThe training objective for the end-to-end R^3 QA system is to minimize the negative log-likelihood of obtaining the correct answer $y$ given a question $x$,\\n\\n$$\\n\\\\begin{aligned}\\n\\\\mathcal{J}(\\\\theta) &= -\\\\mathbb{E}_{z\\\\sim\\\\pi(.\\\\vert x)} [L(y \\\\vert z, x)] \\\\\\\\\\n\\\\nabla \\\\mathcal{J}(\\\\theta) \\n&= - \\\\nabla_\\\\theta \\\\sum_z \\\\pi(z \\\\vert x) L(y \\\\vert z, x) \\\\\\\\\\n&= - \\\\sum_z \\\\big( L(y \\\\vert z, x) \\\\nabla_\\\\theta\\\\pi(z \\\\vert x) + \\\\pi(z \\\\vert x) \\\\nabla_\\\\theta L(y \\\\vert z, x) \\\\big) \\\\\\\\\\n&= - \\\\mathbb{E}_{z\\\\sim\\\\pi(.\\\\vert x)} \\\\big( \\\\color{red}{L(y \\\\vert z, x)\\\\nabla_\\\\theta\\\\log\\\\pi(z \\\\vert x)} + \\\\nabla_\\\\theta L(y \\\\vert z, x) \\\\big) \\\\\\\\\\n&\\\\approx - \\\\mathbb{E}_{z\\\\sim\\\\pi(.\\\\vert x)} \\\\big( \\\\underbrace{\\\\color{red}{R(y \\\\vert z, x)\\\\nabla_\\\\theta\\\\log\\\\pi(z \\\\vert x)}}_\\\\text{REINFORCE} + \\\\nabla_\\\\theta L(y \\\\vert z, x) \\\\big)\\n\\\\end{aligned}\\n$$\\n\\nEssentially in training, given a passage $z$ sampled by the retriever, the reader is trained by gradient descent while the retriever is trained by REINFORCE using $L(y \\\\vert z, x)$ as the reward function. However, $L(y \\\\vert z, x)$ is not bounded and may introduce a lot of variance. The paper replaces the reward with a customized scoring function by comparing the ground truth $y$ and the answer extracted by the reader $\\\\hat{y}$:\\n\\n$$\\nR(y, \\\\hat{y} \\\\vert z) = \\\\begin{cases}\\n2 & \\\\text{if } y = \\\\hat{y}\\\\\\\\\\nf1(y, \\\\hat{y}) & \\\\text{if } y \\\\cap \\\\hat{y} = \\\\varnothing \\\\\\\\\\n-1 & \\\\text{otherwise}\\n\\\\end{cases}\\n$$\\n\\n\\nFig. 7. The workflow of R^3 training process. (Image source: acl2020-openqa-tutorial/slides/part4)\\nORQA (‚ÄúOpen-Retrieval Question-Answering‚Äù; Lee et al., 2019) jointly learns a retriever + reader QA model to optimize marginal log-likelihood of obtaining correct answers in a supervised manner. No explicit ‚Äúblack-box‚Äù IR system is involved. Instead, it is capable of retrieving any text in an open corpus. During training, ORQA does not need ground-truth context passages (i.e. reading comprehension datasets) but only needs (question, answer) string pairs. Both retriever and reader components are based on BERT, but not shared.\\n\\nFig. 8. An illustration of the retriever component in ORQA. (Image source: replotted based on one slide in acl2020-openqa-tutorial/slides/part5)\\nAll the evidence blocks are ranked by a retrieval score, defined as the inner product of BERT embedding vectors of the [CLS] token of the question $x$ and the evidence block $z$. Note that the encoders for questions and context are independent.\\n\\n$$\\n\\\\begin{aligned}\\nh_x &= \\\\mathbf{W}_x \\\\text{BERT}_x(x)^{\\\\mathtt{[CLS]}} \\\\\\\\\\nh_z &= \\\\mathbf{W}_z \\\\text{BERT}_z(z)^{\\\\mathtt{[CLS]}} \\\\\\\\\\nS_\\\\text{retr}(z, x) &= h_x^\\\\top h_z\\n\\\\end{aligned}\\n$$\\n\\nThe retriever module is pretrained with Inverse Cloze Task (ICT), which is to predict the context given a sentence, opposite to the standard Cloze Task. The ICT objective is to maximize the retrieval score of the correct context $z$ given a random sentence $x$:\\n\\n$$\\nL_\\\\text{ICT} = p_\\\\text{early}(z \\\\vert x) = \\\\frac{\\\\exp(S_\\\\text{retr}(z, x))}{\\\\sum_{z\\'\\\\in\\\\text{BATCH}(\\\\mathcal{Z})} \\\\exp(S_\\\\text{retr}(z\\', x))}\\n$$\\n\\nwhere $\\\\text{BATCH}(\\\\mathcal{Z})$ is the set of evidence blocks in the same batch used as sampled negatives.\\nAfter such pretraining, the BERT retriever is expected to have representations good enough for evidence retrieval. Only the question encoder needs to be fine-tuned for answer extraction. In other words, the evidence block encoder (i.e., $\\\\mathbf{W}_z$ and $\\\\text{BERT}_z$) is fixed and thus all the evidence block encodings can be pre-computed with support for fast Maximum Inner Product Search (MIPS).\\n\\nFig. 9. An illustration of the reader component in ORQA. (Image source: acl2020-openqa-tutorial/slides/part5)\\nThe reader follows the same design as in the original BERT RC experiments. It learns in a supervised manner, while the parameters of the evidence block encoder are fixed and all other parameters are fine-tuned. Given a question $x$ and a gold answer string $y$, the reader loss contains two parts:\\n\\n$$\\n\\\\mathcal{L}(x, y) = \\\\mathcal{L}_\\\\text{early}(x, y) + \\\\mathcal{L}_\\\\text{full}(x, y)\\n$$\\n\\n(1) Find all correct text spans within top $k$ evidence blocks and optimize for the marginal likelihood of a text span $s$ that matches the true answer $y$:\\n\\n$$\\n\\\\begin{aligned}\\nh_s &= \\\\text{BERT}_R(x, y)^{(\\\\text{START}(s))} \\\\\\\\\\nh_e &= \\\\text{BERT}_R(x, y)^{(\\\\text{END}(s))} \\\\\\\\\\nS_\\\\text{read}(z, s, x) &= \\\\text{MLP}([h_s; h_e]) \\\\\\\\\\np(z, s \\\\vert x) &= \\\\frac{\\\\exp(S_\\\\text{read}(z, s, x))}{\\\\sum_{z\\'\\\\in\\\\text{TOP}(k)} \\\\sum_{s\\'\\\\in z\\'} \\\\exp(S_\\\\text{read}(z\\', s\\', x))} \\\\\\\\\\nL_\\\\text{full}(x, y) &= - \\\\log \\\\sum_{\\\\substack{z \\\\in \\\\text{TOP}(k)\\\\\\\\ s \\\\in z}} \\\\sum_{y=\\\\text{TEXT}(s)} p(z, s \\\\vert x)\\n\\\\end{aligned}\\n$$\\n\\nwhere $y=\\\\text{TEXT}(s)$ indicates whether the answer $y$ matches the text span $s$. $\\\\text{TOP}(k)$ is the top $k$ retrieved blocks according to $S_\\\\text{retr}(z, x)$. The paper sets $k=5$.\\n(2) At the early stage of learning, when the retriever is not strong enough, it is possible none of the top $k$ blocks contains the answer. To avoid such sparse learning signals, ORQA considers a larger set of $c$ evidence blocks for more aggressive learning. The paper has $c=5000$.\\n\\n$$\\nL_\\\\text{early}(x, y)\\n= -\\\\log \\\\sum_{\\\\substack{z\\\\in \\\\text{TOP}(c)\\\\\\\\y\\\\in\\\\text{TEXT}(z)}} p_\\\\text{early}(z\\\\vert x)\\n= -\\\\log \\\\sum_{\\\\substack{z\\\\in \\\\text{TOP}(c)\\\\\\\\y\\\\in\\\\text{TEXT}(z)}} \\\\frac{\\\\exp(S_\\\\text{retr}(z, x)}{\\\\sum_{z\\'\\\\in\\\\text{TOP}(c)} \\\\exp(S_\\\\text{retr}(z\\', x)}\\n$$\\n\\nSome issues in SQuAD dataset were discussed in the ORQA paper:\\n\\n\" The notable drop between development and test accuracy for SQuAD is a reflection of an artifact in the dataset‚Äîits 100k questions are derived from only 536 documents. Therefore, good retrieval targets are highly correlated between training examples, violating the IID assumption, and making it unsuitable for learned retrieval. We strongly suggest that those who are interested in end-to-end open-domain QA models no longer train and evaluate with SQuAD for this reason.\"\\n\\nREALM (‚ÄúRetrieval-Augmented Language Model pre-training‚Äù; Guu et al., 2020) also jointly trains retriever + reader by optimizing the marginal likelihood of obtaining the true answer:\\n\\n$$\\np(y \\\\vert x) \\n= \\\\sum_{z \\\\in \\\\mathcal{Z}} \\\\underbrace{p(y \\\\vert x, z)}_\\\\text{reader} \\\\underbrace{p(z \\\\vert x)}_\\\\text{retriever}\\n\\\\approx \\\\sum_{z \\\\in \\\\text{TOP}_k(\\\\mathcal{Z})} p(y \\\\vert x, z) p(z \\\\vert x)\\n$$\\n\\n\\nFig. 10. REALM is first unsupervised pre-trained with salient spans masking and then fine-tuned with QA data. (Image source: Guu et al., 2020).\\nREALM computes two probabilities, $p(z \\\\vert x)$ and $p(y \\\\vert x, z)$, same as ORQA. However, different from ICT in ORQA, REALM upgrades the unsupervised pre-training step with several new design decisions, leading towards better retrievals. REALM pre-trains the model with Wikipedia or CC-News corpus.\\n\\nUse salient span masking. Named entities and dates are identified. Then one of these ‚Äúsalient spans‚Äù is selected and masked. Salient span masking is a special case of MLM and works out well for QA tasks.\\nAdd an empty null document. Because not every question demands a context document.\\nNo trivial retrieval. The context document should not be same as the selected sentence with a masked span.\\nApply the same ICT loss as in ORQA to encourage learning when the retrieval quality is still poor at the early stage of training.\\n\\n\\n‚ÄúAmong all systems, the most direct comparison with REALM is ORQA (Lee et al., 2019), where the fine-tuning setup, hyperparameters and training data are identical. The improvement of REALM over ORQA is purely due to better pre-training methods.‚Äù ‚Äî from REALM paper.\\n\\nBoth unsupervised pre-training and supervised fine-tuning optimize the same log-likelihood $\\\\log p(y \\\\vert x)$. Because the parameters of the retriever encoder for evidence documents are also updated in the process, the index for MIPS is changing. REALM asynchronously refreshes the index with the updated encoder parameters every several hundred training steps.\\nBalachandran, et al. (2021) found that REALM is significantly undertrained and REALM++ achieves great EM accuracy improvement (3-5%) by scaling up the model training with larger batch size and more retrieved documents for the reader to process.\\nDPR (‚ÄúDense Passage Retriever‚Äù; Karpukhin et al., 2020, code) argues that ICT pre-training could be too computationally expensive and the ORQA‚Äôs context encoder might be sub-optimal because it is not fine-tuned with question-answer pairs. DPR aims to resolve these two issues by only training a dense dual-encoder architecture for retrieval only from a small number of Q/A pairs, without any pre-training.\\nSame as previous work, DPR uses the dot-product (L2 distance or cosine similarity also works) of BERT representations as retrieval score. The loss function for training the dual-encoder is the NLL of the positive passage, which essentially takes the same formulation as ICT loss of ORQA. Note that both of them consider other passages in the same batch as the negative samples, named in-batch negative sampling. The main difference is that DPR relies on supervised QA data, while ORQA trains with ICT on unsupervised corpus. At the inference time, DPR uses FAISS to run fast MIPS.\\nDPR did a set of comparison experiments involving several different types of negatives:\\n\\nRandom: any random passage from the corpus;\\nBM25: top passages returned by BM25 which don‚Äôt contain the answer but match most question tokens;\\nIn-batch negative sampling (‚Äúgold‚Äù): positive passages paired with other questions which appear in the training set.\\n\\nDPR found that using gold passages from the same mini-batch and one negative passage with high BM25 score works the best. To further improve the retrieval results, DPR also explored a setting where a BM25 score and a dense embedding retrieval score are linearly combined to serve as a new ranking function.\\nOpen-book QA: Retriever-Generator#\\nCompared to the retriever-reader approach, the retriever-generator also has 2 stages but the second stage is to generate free text directly to answer the question rather than to extract start/end position in a retrieved passage. Some paper also refer to this as Generative question answering.\\n\\nFig. 11. The retriever + generator QA framework combines a document retrieval system with a general language model.\\nA pretrained LM has a great capacity of memorizing knowledge in its parameters, as shown above. However, they cannot easily modify or expand their memory, cannot straightforwardly provide insights into their predictions, and may produce non-existent illusion.\\nPetroni et al. (2020) studied how the retrieved relevant context can help a generative language model produce better answers. They found:\\n\\nAugmenting queries with relevant contexts dramatically improves the pretrained LM on unsupervised machine reading capabilities.\\nAn off-the-shelf IR system is sufficient for BERT to match the performance of a supervised ODQA baseline;\\nBERT‚Äôs NSP pre-training strategy is a highly effective unsupervised mechanism in dealing with noisy and irrelevant contexts.\\n\\nThey pair the BERT model with different types of context, including adversarial (unrelated context), retrieved (by BM25), and generative (by an autoregressive language model of 1.4N parameters, trained on CC-NEWS). The model is found to be robust to adversarial context, but only when the question and the context are provided as two segments (e.g. separated by [SEP]). One hypothesis is related to NSP task: ‚ÄúBERT might learn to not condition across segments for masked token prediction if the NSP score is low, thereby implicitly detecting irrelevant and noisy contexts.‚Äù\\nRAG (‚ÄúRetrieval-Augmented Generation‚Äù; Lewis et al., 2020) combines pre-trained parametric (language model) and non-parametric memory (external knowledge index) together for language generation. RAG can be fine-tuned on any seq2seq task, whereby both the retriever and the sequence generator are jointly learned. They found that unconstrained generation outperforms previous extractive approaches.\\nRAG consists of a retriever model $p_\\\\eta(z \\\\vert x)$ and a generator model $p_\\\\theta(y_i \\\\vert x, z, y_{1:i-1})$:\\n\\nThe retriever uses the input sequence $x$ to retrieve text passages $z$, implemented as a DPR retriever. $\\\\log p_\\\\eta(z \\\\vert x) \\\\propto E_z(z)^\\\\top E_x(x)$.\\nThe generator uses $z$ as additional context when generating the target sequence $y$, where the context and the question are simply concatenated.\\n\\nDepending on whether using the same or different retrieved documents for each token generation, there are two versions of RAG:\\n\\n$$\\n\\\\begin{aligned}\\np_\\\\text{RAG-seq}(y \\\\vert x) &= \\\\sum_{z \\\\in \\\\text{TOP}_k(p_\\\\eta(.\\\\vert x))} p_\\\\eta(z \\\\vert x) \\\\prod_i^N p_\\\\theta(y_i \\\\vert x, z, y_{1:i-1}) \\\\\\\\\\np_\\\\text{RAG-token}(y \\\\vert x) &= \\\\prod_i^N \\\\sum_{z \\\\in \\\\text{TOP}_k(p_\\\\eta(.\\\\vert x))} p_\\\\eta(z_i\\\\vert x) p_\\\\theta(y_i \\\\vert x, z_i, y_{1:i-1})\\n\\\\end{aligned}\\n$$\\n\\nThe retriever + generator in RAG is jointly trained to minimize the NLL loss, $\\\\mathcal{L}_\\\\text{RAG} = \\\\sum_j -\\\\log p(y_j \\\\vert x_j)$. Updating the passage encoder $E_z(.)$ is expensive as it requires the model to re-index the documents for fast MIPS. RAG does not find fine-tuning $E_z(.)$ necessary (like in ORQA) and only updates the query encoder + generator.\\n\\nFig. 12. An illustration of retrieval-augmented generation (RAG) architecture. (Image source: Lewis et al., 2020)\\nAt decoding/test time, RAG-token can be evaluated via a beam search. RAG-seq cannot be broken down into a set of per-token likelihood, so it runs beam search for each candidate document $z$ and picks the one with optimal $p_\\\\theta(y_i \\\\vert x, z, y_{1:i-1})$.\\nThe Fusion-in-Decoder approach, proposed by Izacard & Grave (2020) is also based on a pre-trained T5. It works similar to RAG but differently for how the context is integrated into the decoder.\\n\\nRetrieve top $k$ related passage of 100 words each, using BM25 or DPR.\\nEach retrieved passage and its title are concatenated with the question using special tokens like question:, title: and context: to indicate the content differences.\\nEach retrieved passage is processed independently and later combined in the decoder. Processing passages independently in the encoder allows us to parallelize the computation. OTOH, processing them jointly encourages better aggregation of multiple pieces of evidence. The aggregation part is missing in extractive approaches.\\n\\nNote that they did fine-tune the pretrained LM independently for each dataset.\\nClosed-book QA: Generative Language Model#\\nBig language models have been pre-trained on a large collection of unsupervised textual corpus. Given enough parameters, these models are able to memorize some factual knowledge within parameter weights. Therefore, we can use these models to do question-answering without explicit context, just like in a closed-book exam. The pre-trained language models produce free text to respond to questions, no explicit reading comprehension.\\n\\nFig. 13. The amount of computation used for training big language models of different sizes is getting big. (Image source: Brown et al., 2020).\\nRoberts et al. (2020) measured the practical utility of a language model by fine-tuning a pre-trained model to answer questions without access to any external context or knowledge. They fine-tuned the T5 language model (same architecture as the original Transformer) to answer questions without inputting any additional information or context. Such setup enforces the language model to answer questions based on ‚Äúknowledge‚Äù that it internalized during pre-training.\\n\\nFig. 14. T5 is first pre-trained with salient span masking and then fine-tuned for each QA dataset to produce answers in free text. (Image source: Roberts et al. 2020)\\nThe original T5 models were pre-trained on a multi-task mixture including an unsupervised ‚Äúmasked language modeling‚Äù (MLM) tasks on the C4 (‚ÄúColossal Clean Crawled Corpus‚Äù) dataset as well as fine-tuned altogether with supervised translation, summarization, classification, and reading comprehension tasks. Roberts, et al. (2020)  took a pre-trained T5 model and continued pre-training with salient span masking over Wikipedia corpus, which has been found to substantially boost the performance for ODQA. Then they fine-tuned the model for each QA datasets independently.\\nWith a pre-trained T5 language model +  continue pre-training with salient spans masking + fine-tuning for each QA dataset,\\n\\nIt can attain competitive results in open-domain question answering without access to external knowledge.\\nA larger model can obtain better performance. For example, a T5 with 11B parameters is able to match the performance with DPR with 3 BERT-base models, each with 330M parameters.\\n\\nInterestingly, fine-tuning is not strictly necessary. GPT3 (Brown et al., 2020) has been evaluated on the closed book question answering task without any gradient updates or fine-tuning. During evaluation, the few-shot, one-shot and zero-shot settings here only refer to how many demonstrations are provided as context in the text input:\\n\\n‚Äúfew-shot learning‚Äù: GPT3 is allowed to take as many demonstrations as what can fit into the model‚Äôs context window (typically 10 to 100).\\n‚Äúone-shot learning‚Äù: only one demonstration is provided.\\n‚Äúzero-shot learning‚Äù: no demonstrations are allowed and only an instruction in natural language is given to the model.\\n\\nThe performance grows with the model size. On the TriviaQA dataset, GPT3 evaluation with demonstrations can match or exceed the performance of SOTA baseline with fine-tuning.\\n\\nFig. 15. GPT3\\'s performance on TriviaQA grows smoothly with the model size. More demonstrations lead to better performance. (Image source: Brown et al., 2020).\\nCheck out this cool example in OpenAI API playground viewer. The model is able to answer factal questions in short answer and not to make up things when the model does not know the answer. I added the last two questions and asked the model to respond with A:. The API is still in beta version, so you might need to apply to get on the wait list.\\nQ: Who is Batman?\\nA: Batman is a fictional comic book character.\\n##\\nQ: What is torsalplexity?\\nA: ?\\n##\\nQ: What is Devz9?\\nA: ?\\n##\\nQ: Who is George Lucas?\\nA: George Lucas is American film director and producer famous for creating Star Wars.\\n##\\nQ: What is the capital of California?\\nA: Sacramento.\\n##\\nQ: What orbits the Earth?\\nA: The Moon.\\n##\\nQ: Who is Fred Rickerson?\\nA: ?\\n##\\nQ: What is an atom?\\nA: An atom is a tiny particle that makes up everything.\\n##\\nQ: Who is Alvan Muntz?\\nA: ?\\n##\\nQ: What is Kozar-09?\\nA: ?\\n##\\nQ: How many moons does Mars have?\\nA: Two, Phobos and Deimos.\\n##\\nQ: What is COVID-19?\\nA: ?\\n##\\nQ: What is H1N1?\\nA: H1N1 is a strain of influenza.\\nRelated Techniques#\\nFast Maximum Inner Product Search (MIPS)#\\nMIPS (maximum inner product search) is a crucial component in many open-domain question answering models. In retriever + reader/generator framework, a large number of passages from the knowledge source are encoded and stored in a memory. A retrieval model is able to query the memory to identify the top relevant passages which have the maximum inner product with the question‚Äôs embedding.\\nWe need fast MIPS because the number of precomputed passage representations can be gigantic. There are several ways to achieve fast MIPS at run time, such as asymmetric LSH, data-dependent hashing,  and FAISS.\\nLanguage Model Pre-training#\\nTwo pre-training tasks are especially helpful for QA tasks, as we have discussed above.\\n\\n\\nInverse Cloze Task  (proposed by ORQA): The goal of Cloze Task is to predict masked-out text based on its context. The prediction of Inverse Cloze Task (ICT) is in the reverse direction, aiming to predict the context given a sentence. In the context of QA tasks, a random sentence can be treated as a pseudo-question, and its context can be treated as pseudo-evidence.\\n\\n\\nSalient Spans Masking (proposed by REALM): Salient span masking is a special case for MLM task in language model training. First, we find salient spans by using a tagger to identify named entities and a regular expression to identify dates. Then one of the detected salient spans is selected and masked. The task is to predict this masked salient span.\\n\\n\\nSummary#\\n\\n\\n\\nModel\\nRetriever\\nReader / Generator\\nPre-training / Fine-tuning\\nEnd2end\\n\\n\\n\\n\\nDrQA\\nTF-IDF\\nBi-directional LSTM\\n‚Äì\\nNo\\n\\n\\nBERTserini\\nAserini + BM25\\nBERT without softmax layer\\nFine-tune with SQuAD\\nNo\\n\\n\\nMulti-passage BERT\\nElasticSearch + BM25\\nMulti-passage BERT + Passage ranker\\n\\nNo\\n\\n\\nR^3\\nClassic IR + Match-LSTM\\nMatch-LSTM\\n\\nYes\\n\\n\\nORQA\\nDot product of BERT embeddings\\nBERT-RC\\nInverse cloze task\\nYes\\n\\n\\nREALM\\nDot product of BERT embeddings\\nBERT-RC\\nSalient span masking\\nYes\\n\\n\\nDPR\\nDot product of BERT embeddings\\nBERT-RC\\nsupervised training with QA pairs\\nYes\\n\\n\\nDenSPI\\nClassic + Neural IR\\n‚Äì\\n\\nYes\\n\\n\\nT5 + SSM\\n‚Äì\\nT5\\nSSM on CommonCrawl data + Fine-tuning on QA data\\nYes\\n\\n\\nGPT3\\n‚Äì\\nGPT3\\nNSP on CommonCrawl data\\nYes\\n\\n\\nRAG\\nDPR retriever\\nBART\\n\\nYes\\n\\n\\nFusion-in-Decoder\\nBM25 / DPR retriever\\nTranformer\\n\\nNo\\n\\n\\n\\n\\nFig. 16. A comparison of performance of several QA models on common QA datasets. On TriviaQA, two columns of results are reported, on the open domain test set (left) and on the hidden test set (right). (Image source: Izacard & Grave, 2020).\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Oct 2020). How to build an open-domain question answering system? Lil‚ÄôLog. https://lilianweng.github.io/posts/2020-10-29-odqa/.\\n\\nOr\\n@article{weng2020odqa,\\n  title   = \"How to Build an Open-Domain Question Answering System?\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2020\",\\n  month   = \"Oct\"\\n  url     = \"https://lilianweng.github.io/posts/2020-10-29-odqa/\"\\n}\\nAppendix: QA Datasets#\\n\\nSQuAD 2.0: the Stanford QA dataset.\\nRACE: a reading comprehension dataset collected from English Examinations that are created for middle school and high school students.\\nTREC QA: the TREC QA collections.\\nMS MARCO: a QA dataset featuring 100,000 real Bing questions and a human generated answer.\\nCuratedTREC: based on the benchmarks from the TREC QA tasks that have been curated by Baudis & Sedivy (2015).\\nGoogle Natural Questions:  contains real user questions issued to Google search, and answers found from Wikipedia by annotators.\\nWebQuestions: designed for knowledge-base QA with answers restricted to Freebase entities.\\nWikiQA: Bing query logs were used as the source of questions. Each question is then linked to a Wikipedia page that potentially contains the answer.\\nWikiMovies: contains movie-related questions from the OMDb and MovieLens databases and where the questions can be answered using Wikipedia pages.\\nWikiReading: to predict textual values from the structured knowledge base Wikidata by reading the text of the corresponding Wikipedia articles.\\nTriviaQA: a reading comprehension dataset containing 95K question-answer pairs authored by trivia enthusiasts and independently gathered multiple evidence documents per question.\\n Jeopardy! Questions: contains 200,000+ Jeopardy! questions.\\nDeepMind Q&A Dataset: question/answer pairs from CNN and Daily Mail articles.\\nbAbi: a rich collection of datasets for text understanding by Facebook.\\nFEVER: for fact extraction and verification.\\nSearchQA: question-answer pairs were crawled from from  J! Archive, and then augmented with text snippets from Google.\\nQuasar-T: a collection of open-domain trivia questions and their answers obtained from various internet sources.\\nQuiz bowl: contains data from a trivia competition called quiz bowl.\\nAmbigNQ: ambiguous questions selected from NQ-OPEN dataset.\\nQA-Overlap: a collections of overlapped answers/questions between train and test set for Natural Questions, TriviaQA, and WebQuestions.\\n\\nReferences#\\n[1] Danqi Chen & Scott Yih. ‚ÄúACL2020 Tutorial: Open-Domain Question Answering‚Äù July 2020.\\n[2] Danqi Chen, et al. ‚ÄúReading Wikipedia to Answer Open-Domain Questions‚Äù ACL 2017. | code\\n[3] Shuohang Wang, et al. ‚ÄúR^3: Reinforced Ranker-Reader for Open-Domain Question Answering‚Äù AAAI 2018.\\n[4] Jimmy Lin. ‚ÄúThe neural hype and comparisons against weak baselines.‚Äù ACM SIGIR Forum. Vol. 52. No. 2. 2019.\\n[5] Wei Yang, et al. ‚ÄúEnd-to-End Open-Domain Question Answering with BERTserini‚Äù NAACL 2019.\\n[6] Christopher Clark & Matt Gardner. ‚ÄúSimple and Effective Multi-Paragraph Reading Comprehension.‚Äù arXiv:1710.10723 (2017).\\n[7] Rodrigo Nogueira & Kyunghyun Cho. ‚ÄúPassage Re-ranking with BERT.‚Äù arXiv preprint arXiv:1901.04085 (2019). | code\\n[8] Zhiguo Wang, et al. ‚ÄúMulti-passage BERT: A globally normalized BERT model for open-domain question answering.‚Äù EMNLP 2019.\\n[9] Minjoon Seo et al. ‚ÄúReal-time open-domain question answering with dense-sparse phrase index.‚Äù ACL 2019.\\n[10] Kenton Lee, et al. ‚ÄúLatent Retrieval for Weakly Supervised Open Domain Question Answering‚Äù ACL 2019.\\n[11] Kelvin Guu, et al. ‚ÄúREALM: Retrieval-Augmented Language Model Pre-Training‚Äù arXiv:2002.08909 (2020).\\n[12] Vladimir Karpukhin et al. ‚ÄúDense passage retrieval for open-domain question answering.‚Äù. EMNLP 2020. | code\\n[13] Patrick Lewis et al. ‚ÄúRetrieval-Augmented Generation for Knowledge-Intensive NLP Tasks‚Äù arXiv:2005.11401 (2020).\\n[14] Adam Roberts, et al. ‚ÄúHow Much Knowledge Can You Pack Into the Parameters of a Language Model?‚Äù EMNLP 2020.\\n[15] Tom Brown, et al. ‚ÄúLanguage models are few-shot learners.‚Äù arXiv:2005.14165 (2020).\\n[16] Fabio Petroni, et al. ‚ÄúHow Context Affects Language Models‚Äô Factual Predictions‚Äù AKBC 2020.\\n[17] Gautier Izacard & Edouard Grave. ‚ÄúLeveraging passage retrieval with generative models for open domain question answering.‚Äù arXiv:2007.01282 (2020).\\n[18] ‚ÄúDive into deep learning: Beam search‚Äù\\n[19] Patrick Lewis, et al. ‚ÄúQuestion and Answer Test-Train Overlap in Open-Domain Question Answering Datasets‚Äù arXiv:2008.02637 (2020). | data\\n[20] Herv√© Jegou, et al. ‚ÄúFaiss: A library for efficient similarity search‚Äù Mar 2017.\\n[21] Vidhisha Balachandran, et al. ‚ÄúSimple and Efficient ways to Improve REALM.‚Äù arXiv:2104.08710 (2021).\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "web_loader = WebBaseLoader('https://lilianweng.github.io/posts/2020-10-29-odqa/',\n",
    "                           bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "                              class_=('post-title', \"post-content\", \"post-header\"))),\n",
    "                           )\n",
    "text_documnet = web_loader.load()\n",
    "dict(text_documnet[0])['page_content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pdf Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=' 1 Chapter One \\nIntroduction to Computer  \\n \\nComputer  \\nA computer is an electronic device, operating under the control of instructions stored \\nin its own memory that can accept data (input), process the data according to specified \\nrules, produce information (output), and store the information for future use1. \\n \\nFunctionalities of a computer2  \\nAny digital computer carries out five functions in gross terms:  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nComputer Components  \\nAny kind of computers consists of HARDWARE AND SOFTWARE . \\n \\nHardware:  \\nComputer hardware is the collection of  physical elements that constitutes a computer \\nsystem. Computer hardware refers to the physical parts or components of a computer \\nsuch as the monitor, mouse, keyboard, computer data storage, hard drive disk (HDD), \\nsystem unit (graphic cards, sound cards, m emory, motherboard and chips), etc. all of \\nwhich are physical objects that can be touched .3 \\n                                                           \\n1 Vermaat , Misty E. Microsoft Office 2013 Introductory. Cengage Learning, p.IT3.  2014  \\n2 http://www.tutorialspoint.com/computer_fundamentals/computer_quick_guide.htm  \\n3 http://en.wikipedia.org/wiki/Computer_hardware   \\n', metadata={'source': 'https://www.just.edu.jo/~mqais/CIS99/PDF/Ch.01_Introduction_%20to_computers.pdf', 'page': 0}),\n",
       " Document(page_content=' 2  \\nInput Devices  \\nInput device is any peripheral (piece of computer hardware equipment to provide data \\nand control signals to an information processing system such as a computer or other \\ninformation appliance.  \\nInput device Translate data from form  that humans understand to one that the computer \\ncan work with . Most common are keyboard and mouse  \\n \\n \\nExample of Input Devices: - \\n1. Keyboard  2. Mouse (pointing device)  3. Microphone  \\n4. Touch screen  5. Scanner  6. Webcam  \\n7. Touchpads  8. MIDI keyboard  9.  \\n10. Graphics Tablets  11. Cameras  12. Pen Input  \\n13. Video Capture Hardware  14. Microphone  15. Trackballs  \\n16. Barcode reader  17. Digital camera  18. Joystick  \\n19. Gamepad  20. Electronic Whiteboard  21.  \\n \\nNote: The most common use keyboard is the QWERTY keyboard . Generally standard  Keyboard \\nhas 104 keys .  \\n \\n  \\n', metadata={'source': 'https://www.just.edu.jo/~mqais/CIS99/PDF/Ch.01_Introduction_%20to_computers.pdf', 'page': 1}),\n",
       " Document(page_content=' 3 Central Processing Unit (CPU)   \\nA CPU is brain of a computer. It is responsible for  all functions and processes. \\nRegarding computing power, the CPU is the most important element of a computer \\nsystem.  \\n \\nThe CPU is comprised of three main parts  : \\n* Arithmetic  Logic  Unit (ALU) : Executes all arithmetic and logical operations.  \\nArithmetic calculations like as addition, subtraction, multiplication and division . \\nLogical operation like compare numbers, letters, or special characters  \\n* Control  Unit (CU):  controls and co -ordinates computer components.  \\n1. Read the code for the next instruction to be executed.  \\n2. Increment the program counter so it points to the next instruction.  \\n3. Read whatever data the instruction requires from cells in memory.  \\n4. Provide the necessary data to an ALU or register.  \\n5. If the instruction requires an ALU or specialized hardware to complete, instruct \\nthe hardware to perform the requested operation.  \\n* Registers  :Stores  the data that is to be executed  next, \" very fast storage area \". \\n \\nPrimary Memory: - \\n1. RAM : Random Access Memory (RAM) is a memory scheme within the computer \\nsystem responsible for storing data on a temporary basis, so that it can be promptly \\naccessed by the processor as and when needed. It is volatile in nature, which means \\nthat data will be e rased once supply to the storage device is turned off. RAM stores \\ndata randomly and the processor accesses these data randomly from the RAM \\nstorage. RAM is considered \"random access\" because you can access any memory \\ncell directly if you know the row and c olumn that intersect at that cell.  \\n2. ROM  (Read Only Memory): ROM is a permanent form of storage. ROM stays \\nactive regardless of whether power supply to it is turned on or off . ROM devices \\ndo not allow data stored on them to be modified.  \\n \\nSecondary Memory: -  \\nStores data and programs  permanently   :its retained after the  power is turned off  \\n \\n1. Hard drive (HD): A hard disk is part of a unit, often called a \"disk drive,\" \"hard drive,\" or \"hard \\ndisk drive,\" that store and provides relatively quick access to large amounts of data on an \\nelectromagnetically char ged surface or set of surfaces.  \\n2. Optical Disk: an optical disc drive (ODD) is a disk drive that uses laser light as part of the process \\nof reading or writing data to or from optical discs. Some drives c an only read from discs, but recent \\ndrives are commonly both readers and recorders, also called burners or writers. Compact discs, \\nDVDs, and Blu -ray discs are common types of optical media which can be read and recorded by \\nsuch drives. Optical drive is the  generic name; drives are usually described as \"CD\" \"DVD\", or \\n\"Bluray\", followed by \"drive\", \"writer\", etc. There are three main types of optical media: CD, \\nDVD, and Blu -ray disc. CDs can store up to 700 megabytes (MB) of data and DVDs can store up \\nto 8.4 GB of data. Blu -ray discs, which are the newest type of optical media, can store up to 50 \\nGB of data. This storage capacity is a clear advantage over the floppy disk storage media (a \\nmagnetic media), which only has a capacity of 1.44 MB.  ', metadata={'source': 'https://www.just.edu.jo/~mqais/CIS99/PDF/Ch.01_Introduction_%20to_computers.pdf', 'page': 2}),\n",
       " Document(page_content=' 4 3. Flash Disk  \\nA storage module made of flash memory chips. A Flash disks have no mechanical platters or access \\narms, but the term \"disk\" is used because the data are accessed as if they were on a hard drive. The \\ndisk storage structure is emulated.  \\n \\n \\n \\nOutput devices  \\nAn output device is any piece of computer hardware equipment used to communicate \\nthe results of data processing carried out by an information processing system (such \\nas a computer) which converts the electronically generated information into human -\\nreadable fo rm. \\n \\nExample on Output Devices:  \\n1. Monitor  2. LCD Projection Panels  \\n3. Printers (all types)  4. Computer Output Microfilm (COM)  \\n5. Plotters  6. Speaker(s)  \\n7. Projector   \\n \\nNote Basic types of monitors are a.Cathode Ray Tube (CRT). B. Liquid Crystal Displays (LCD). \\nc.light -emitting diod e (LED).  \\nPrinter types: 1 -Laser Printer. 2 -Ink Jet Printer. 3 -Dot Matrix Printer  \\n \\n \\n', metadata={'source': 'https://www.just.edu.jo/~mqais/CIS99/PDF/Ch.01_Introduction_%20to_computers.pdf', 'page': 3}),\n",
       " Document(page_content=' 5 Software  \\nSoftware is a generic term for organized  collections of computer data and  instructions, \\noften broken into two major  categories: system software that  provides the basic non -\\ntask-specific  functions of the computer, and application software which is used  by \\nusers to accomplish specific tasks.   \\n \\nSoftware Types  \\nA. System software  is responsible for controlling, integrating, and  managing the \\nindividual hardw are components of a computer system so  that other software and \\nthe users of the system see it as a functional unit  without having to be concerned \\nwith the low -level details such as  transferring data from memory to disk, or \\nrendering text onto a display.  Generally, system software consists of an operating \\nsystem and some  fundamental utilities such as disk formatters, file managers, \\ndisplay  managers, text editors, user authentication (login) and management tools,  \\nand networking and device control software.   \\nB. Application software  is used to accomplish specific tasks other than  just running \\nthe computer system. Application software may consist of a  single program, such \\nas an image viewer; a small collection of programs  (often called a software \\npackage) that work closely together to accomplish a  task, such as a spreadsheet or \\ntext processing system; a larger collection  (often called a software suite) of related \\nbut independent programs and  packages that have a common user interface or \\nshared data format, such  as Microsoft Office, which consists of closely integrated \\nword processor,  spreadsheet, database, etc.; or a software system, such as a \\ndatabase  management system, which is a collection of fundamental programs that  \\nmay provide some service to a variety of o ther independent applications.  \\nComparison Application Software and System Software  \\n System Software  Application Software  \\n Computer software, or just software is a \\ngeneral term primarily used for digitally stored \\ndata such as computer programs and other \\nkinds of information read and written by \\ncomputers. App comes under computer \\nsoftware though it has a wide scope now.  Application software, also known as an \\napplication or an \"app\", is computer software \\ndesigned to help the user to perform specific \\ntasks.  \\nExample:  \\n 1) Microsoft Windows  \\n2) Linux  \\n3) Unix  \\n4) Mac OSX  \\n5) DOS  1) Opera (Web Browser)  \\n2) Microsoft Word (Word Processing)  \\n3) Microsoft Excel (Spreadsheet software)  \\n4) MySQL (Database Software)  \\n5) Microsoft PowerPoint (Presentation Software)  \\n6) Adobe Ph otoshop (Graphics Software)  \\nInteraction:  \\n Generally, users do not interact with system \\nsoftware as it works in the background.  Users always interact with application software \\nwhile doing different activities.  \\n \\nDependency:  \\n System software can run independently of the \\napplication software.  Application software cannot run without the \\npresence of the system software.  \\n ', metadata={'source': 'https://www.just.edu.jo/~mqais/CIS99/PDF/Ch.01_Introduction_%20to_computers.pdf', 'page': 4}),\n",
       " Document(page_content=' 6 Unit of Measurements  \\nStorage measurements : The basic unit used in computer data storage is called  a bit \\n(binary digit). Computers use these little bits, which are composed of ones \\nand zeros, to do things and talk to other computers. All your files, for \\ninstance, are kept in the computer as binary files and translated into words \\nand pictures by the sof tware (which is also ones and zeros). This two \\nnumber system, is called a ‚Äúbinary number system‚Äù since it has only two \\nnumbers in it. The decimal number system in contrast has ten unique digits, \\nzero through nine.  \\nComputer Storage units  \\nBit BIT 0 or 1  \\nKilobyte  KB 1024 bytes  \\nMegabyte  MB 1024 kilobytes  \\nGigabyte  GB 1024 megabytes  \\nTerabyte  TB 1024 gigabytes  \\nSize example  \\n‚Ä¢ 1 bit - answer to an yes/no question  \\n‚Ä¢ 1 byte - a number from 0 to 255.  \\n‚Ä¢ 90 bytes: enough to store a typical line of text from a book.  \\n‚Ä¢ 4 KB: about one page of text.  \\n‚Ä¢ 120 KB: the text of a typical pocket book.  \\n‚Ä¢ 3 MB - a three minute song (128k bitrate)  \\n‚Ä¢ 650 -900 MB - an CD -ROM  \\n‚Ä¢ 1 GB -114 minutes of uncompressed CD -quality audio at 1.4 Mbit/s  \\n‚Ä¢ 8-16 GB - size of a normal flash dr ive \\n \\nSpeed measurement : The speed of Central Processing Unit (CPU) is measured by \\nHertz  (Hz), Which represent a CPU cycle. The speed of CPU is known as  Computer \\nSpeed.   \\nCPU SPEED MEASURES  \\n1 hertz or Hz  1 cycle per second  \\n1 MHz  1 million cycles per second  or 1000 Hz  \\n1 GHz  1 billion cycles per second or 1000 MHz  \\n ', metadata={'source': 'https://www.just.edu.jo/~mqais/CIS99/PDF/Ch.01_Introduction_%20to_computers.pdf', 'page': 5}),\n",
       " Document(page_content=' 7 Computers classification***   \\nComputers can be generally classified by size and power as follows, though there is  \\nConsiderable overlap:  \\n‚Ä¢ Personal computer : A small, single -user computer based on  a microprocessor. In \\naddition to the microprocessor, a personal computer has a keyboard for entering \\ndata, a monitor for displaying information, and a storage device for saving data.  \\n‚Ä¢ workstation : A powerful, single -user computer. A workstation is like  a personal \\ncomputer, but it has a more powerful microprocessor and a higher -quality \\nmonitor.  \\n‚Ä¢ minicomputer : A multi -user computer capable of supporting from 10 to hundreds of  \\nusers simultaneously.  \\n‚Ä¢ mainframe : A powerful multi -user computer capable of  supporting many hundreds \\nor thousands of users simultaneously.  \\n‚Ä¢ supercomputer : An extremely fast computer that can perform hundreds of millions \\nof instructions per second.  \\n \\nLaptop and Smartphone Computers  \\nLAPTOP : A laptop is a battery or AC -powered  personal computer that can be easily  \\ncarried and used in a variety of locations.  Many laptops are designed to \\nhave all of the  functionality of a desktop computer, whichmeans they can \\ngenerally run the same software and open the same  types of files. However , \\nsome laptops, such as netbooks, sacrifice  some functionality in order to be \\neven more portable.   \\n \\n \\n \\nNetbook : A netbook is a type of laptop that is  designed to be even more portable.  \\nNetbooks are often cheaper than laptops  or desktops. They are generally less \\npowerful than other types of computers,  but they provide enough power for \\nemail and internet access, which is  where the name \"netbook\" comes from.  \\nMobile Device : A mobile device is basically any handheld computer. It is designed to \\nbe extremely portab le, often fitting in the palm of your hand or in your pocket.  \\nSome mobile devices are more powerful, and they allow you to do many  of \\n                                                           \\n ***http://www.acobas.net/teaching/survival/hand outs/pcwebopedia.pdf  ', metadata={'source': 'https://www.just.edu.jo/~mqais/CIS99/PDF/Ch.01_Introduction_%20to_computers.pdf', 'page': 6}),\n",
       " Document(page_content=' 8 the same things you can do with a desktop or laptop computer. These  include \\ntablet computers, e -readers, and smartphones.   \\n \\nTablet Computers : Like laptops, tablet computers are  designed to be portable. \\nHowever, they  provide a very different computing  experience. The most \\nobvious difference  is that tablet computers don\\'t have  keyboards or touchpads. \\nInstead, the  entire screen is touch -sensitive, allowing you to type on a virtual \\nkeyboard  and use your finger as a mouse pointer.  Tablet computers are mostly \\ndesigned for consuming media, and they are  optimized for tasks like web \\nbrowsing, watching videos, reading e -books,  and playi ng games. For many \\npeople, a \"regular\" computer like  a desktop or laptop is still needed in order \\nto use some programs.  However, the convenience of a tablet computer means \\nthat it may be  ideal as a second computer.   \\n \\nSmartphones : A smartphone is a powerful  mobile  phone that is designed to run a  \\nvariety of applications in addition to  phone service. They are basically  small \\ntablet computers, and they  can be used for web  browsing, watching  videos, \\nreading e -books, playing games and more.   \\n \\nData, Information an d Knowledge  \\nData : Facts and figures which relay something specific, but which are not  organized \\nin any way and which provide no further information regarding  patterns, context, etc. \\nSo data means \"unstructured facts and figures that  have the least impact o n the typical \\nmanager.\"   \\n \\nInformation : For data to become information, it must be contextualized,  categorized, \\ncalculated and condensed. Information thus paints a bigger  picture; it is data with \\nrelevance and purpose. It may convey a trend in the  environme nt, or perhaps indicate \\na pattern of sales for a given period of  time. Essentially information is found \"in \\nanswers to questions that begin  with such words as who, what, where, when, and how \\nmany\".  \\n \\nKnowledge : Knowledge is closely linked to doing and implies know -how and \\nunderstanding. The knowledge possessed by each individual is a  product of his \\nexperience, and encompasses the norms by which he  evaluates new inputs from his \\nsurroundings . \\n  \\n ', metadata={'source': 'https://www.just.edu.jo/~mqais/CIS99/PDF/Ch.01_Introduction_%20to_computers.pdf', 'page': 7}),\n",
       " Document(page_content=' 9 The content of the human mind can be  classified into four categories:  \\n1. Data: symbols  \\n2. Information: data that are processed to be useful; provides answers to  \"who\", \"what\", \\n\"where\", and \"when\" questions  \\n3. Knowledge: application of data and information; answers \"how\"  questions  \\n4. Wisdom: evaluated understanding.  \\nWe need to understand that processing data produced Information and process  \\nInformation produces Knowledge and so on   \\nCharacteristics of Computer  \\nSpeed, accuracy, diligence, storage capability a nd versatility are some of the key  \\ncharacteristics of a computer. A brief overview of these characteristics are   \\n‚Ä¢ Speed : The computer can process data very fast, at the rate of millions of  instructions \\nper second. Some calculations that would have taken h ours and days to \\ncomplete otherwise, can be completed in a few seconds using  the computer. \\nFor example, calculation and generation of salary slips of  thousands of \\nemployees of an organization, weather forecasting that  requires analysis of a \\nlarge amount of  data related to temperature, pressure  and humidity of various \\nplaces, etc.  \\n‚Ä¢ Accuracy : Computer provides a high degree of accuracy. For example, the  computer \\ncan accurately give the result of division of any two numbers up  to 10 decimal \\nplaces.  \\n‚Ä¢ Diligenc e: When used for a longer period of time, the computer does not  get tired or \\nfatigued. It can perform long and complex calculations with the  same speed and \\naccuracy from the start till the end.   \\n‚Ä¢ Storage Capability : Large volumes of data and information can be stored in  the \\ncomputer and also retrieved whenever required. A limited amount of  data can \\nbe stored, temporarily, in the primary memory. Secondary storage  devices like \\nfloppy disk and compact disk can store a l arge amount of data  permanently.  \\n‚Ä¢ Versatility : Computer is versatile in nature. It can perform different types of  tasks \\nwith the same ease. At one moment you can use the computer to  prepare a letter \\ndocument and in the next moment you may play music or  print a document.  \\nComputers have several limitations too. Computer can only perform tasks that \\nit has  been programmed to do.  \\n  ', metadata={'source': 'https://www.just.edu.jo/~mqais/CIS99/PDF/Ch.01_Introduction_%20to_computers.pdf', 'page': 8}),\n",
       " Document(page_content=' 10 Computer cannot do any work without instructions from  the user. It executes \\ninstructions as specified by the user and does not take  its own  decisions.  \\nComputer Viruses* \\nViruses: A virus is a small piece of software that piggybacks on real programs. For \\nexample, a virus might attach itself to a program such as a spreadsheet program. \\nEach time the spreadsheet program runs, the virus ru ns, too, and it has the \\nchance to reproduce (by attaching to other programs) or wreak havoc.  \\n‚Ä¢E-mail viruses: An e -mail virus travels as an attachment to e -mail messages, and \\nusually replicates itself by automatically mailing itself to dozens of people \\nin the victim\\'s e -mail address book. Some e -mail viruses don\\'t even require \\na double -click -- they launch when you view the infected message in the \\npreview pane of your e -mail software [source: Johnson].  \\n‚Ä¢Trojan horses: A Trojan horse is simply a computer program. The program claims to \\ndo one thing (it may claim to be a game) but instead does damage when you \\nrun it (it may erase your hard disk). Trojan horses have no way to replicate \\nautomatically.  \\n‚Ä¢Worms: A worm is a small piece of software that uses computer networks and security \\nholes to replicate itself. A copy of the worm scans the network for an other \\nmachine that has a specific security hole. It copies itself to the new machine \\nusing the security hole, and then starts replicating from there, as well.  \\nWhat  are some  tips to avoid  viruses  and lessen  their  impact ?* \\n\\uf0b7 Install anti -virus software from  a reputable vendor. Update it and use it \\nregularly . \\n\\uf0b7 In addition to scanning for viruses on a regular basis, install an \"on access\" \\nscanner (included in most anti -virus software packages) and configure it to start \\neach time you start up your computer. This  will protect your system by \\nchecking for viruses each time you run an executable file . \\n\\uf0b7 Use a virus scan before you open any new programs or files that may contain \\nexecutable code. This includes packaged software that you buy from the store \\nas well as any program you might download from the Internet . \\n\\uf0b7 If you are a member of an online community or chat room, be very careful \\nabout accepting files or clicking links that you find or that people send you \\nwithin the community . \\n\\uf0b7 Make sure you back up your data (documents, bookmark files, important email \\nmessages, etc.) on disc so that in the event of a virus infection, you do not lose \\nvaluable work . \\n \\n                                                           \\n *http://computer.howstuffworks.com/virus.htm  \\n *http://www.us -cert.gov/publications/virus -basics  ', metadata={'source': 'https://www.just.edu.jo/~mqais/CIS99/PDF/Ch.01_Introduction_%20to_computers.pdf', 'page': 9})]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "pdf_loader = PyPDFLoader('https://www.just.edu.jo/~mqais/CIS99/PDF/Ch.01_Introduction_%20to_computers.pdf')\n",
    "docs = pdf_loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='1 Chapter One \\nIntroduction to Computer  \\n \\nComputer  \\nA computer is an electronic device, operating under the control of instructions stored \\nin its own memory that can accept data (input), process the data according to specified \\nrules, produce information (output), and store the information for future use1. \\n \\nFunctionalities of a computer2  \\nAny digital computer carries out five functions in gross terms:  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nComputer Components  \\nAny kind of computers consists of HARDWARE AND SOFTWARE . \\n \\nHardware:  \\nComputer hardware is the collection of  physical elements that constitutes a computer \\nsystem. Computer hardware refers to the physical parts or components of a computer \\nsuch as the monitor, mouse, keyboard, computer data storage, hard drive disk (HDD), \\nsystem unit (graphic cards, sound cards, m emory, motherboard and chips), etc. all of \\nwhich are physical objects that can be touched .3', metadata={'source': 'https://www.just.edu.jo/~mqais/CIS99/PDF/Ch.01_Introduction_%20to_computers.pdf', 'page': 0}),\n",
       " Document(page_content='1 Vermaat , Misty E. Microsoft Office 2013 Introductory. Cengage Learning, p.IT3.  2014  \\n2 http://www.tutorialspoint.com/computer_fundamentals/computer_quick_guide.htm  \\n3 http://en.wikipedia.org/wiki/Computer_hardware', metadata={'source': 'https://www.just.edu.jo/~mqais/CIS99/PDF/Ch.01_Introduction_%20to_computers.pdf', 'page': 0}),\n",
       " Document(page_content='2  \\nInput Devices  \\nInput device is any peripheral (piece of computer hardware equipment to provide data \\nand control signals to an information processing system such as a computer or other \\ninformation appliance.  \\nInput device Translate data from form  that humans understand to one that the computer \\ncan work with . Most common are keyboard and mouse  \\n \\n \\nExample of Input Devices: - \\n1. Keyboard  2. Mouse (pointing device)  3. Microphone  \\n4. Touch screen  5. Scanner  6. Webcam  \\n7. Touchpads  8. MIDI keyboard  9.  \\n10. Graphics Tablets  11. Cameras  12. Pen Input  \\n13. Video Capture Hardware  14. Microphone  15. Trackballs  \\n16. Barcode reader  17. Digital camera  18. Joystick  \\n19. Gamepad  20. Electronic Whiteboard  21.  \\n \\nNote: The most common use keyboard is the QWERTY keyboard . Generally standard  Keyboard \\nhas 104 keys .', metadata={'source': 'https://www.just.edu.jo/~mqais/CIS99/PDF/Ch.01_Introduction_%20to_computers.pdf', 'page': 1}),\n",
       " Document(page_content='3 Central Processing Unit (CPU)   \\nA CPU is brain of a computer. It is responsible for  all functions and processes. \\nRegarding computing power, the CPU is the most important element of a computer \\nsystem.  \\n \\nThe CPU is comprised of three main parts  : \\n* Arithmetic  Logic  Unit (ALU) : Executes all arithmetic and logical operations.  \\nArithmetic calculations like as addition, subtraction, multiplication and division . \\nLogical operation like compare numbers, letters, or special characters  \\n* Control  Unit (CU):  controls and co -ordinates computer components.  \\n1. Read the code for the next instruction to be executed.  \\n2. Increment the program counter so it points to the next instruction.  \\n3. Read whatever data the instruction requires from cells in memory.  \\n4. Provide the necessary data to an ALU or register.  \\n5. If the instruction requires an ALU or specialized hardware to complete, instruct \\nthe hardware to perform the requested operation.', metadata={'source': 'https://www.just.edu.jo/~mqais/CIS99/PDF/Ch.01_Introduction_%20to_computers.pdf', 'page': 2}),\n",
       " Document(page_content='* Registers  :Stores  the data that is to be executed  next, \" very fast storage area \". \\n \\nPrimary Memory: - \\n1. RAM : Random Access Memory (RAM) is a memory scheme within the computer \\nsystem responsible for storing data on a temporary basis, so that it can be promptly \\naccessed by the processor as and when needed. It is volatile in nature, which means \\nthat data will be e rased once supply to the storage device is turned off. RAM stores \\ndata randomly and the processor accesses these data randomly from the RAM \\nstorage. RAM is considered \"random access\" because you can access any memory \\ncell directly if you know the row and c olumn that intersect at that cell.  \\n2. ROM  (Read Only Memory): ROM is a permanent form of storage. ROM stays \\nactive regardless of whether power supply to it is turned on or off . ROM devices \\ndo not allow data stored on them to be modified.  \\n \\nSecondary Memory: -  \\nStores data and programs  permanently   :its retained after the  power is turned off', metadata={'source': 'https://www.just.edu.jo/~mqais/CIS99/PDF/Ch.01_Introduction_%20to_computers.pdf', 'page': 2})]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)  \n",
    "documents = text_splitter.split_documents(docs)\n",
    "documents[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "db = Chroma.from_documents(documents, OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='1 Chapter One \\nIntroduction to Computer  \\n \\nComputer  \\nA computer is an electronic device, operating under the control of instructions stored \\nin its own memory that can accept data (input), process the data according to specified \\nrules, produce information (output), and store the information for future use1. \\n \\nFunctionalities of a computer2  \\nAny digital computer carries out five functions in gross terms:  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nComputer Components  \\nAny kind of computers consists of HARDWARE AND SOFTWARE . \\n \\nHardware:  \\nComputer hardware is the collection of  physical elements that constitutes a computer \\nsystem. Computer hardware refers to the physical parts or components of a computer \\nsuch as the monitor, mouse, keyboard, computer data storage, hard drive disk (HDD), \\nsystem unit (graphic cards, sound cards, m emory, motherboard and chips), etc. all of \\nwhich are physical objects that can be touched .3', metadata={'source': 'https://www.just.edu.jo/~mqais/CIS99/PDF/Ch.01_Introduction_%20to_computers.pdf', 'page': 0}),\n",
       " Document(page_content='employees of an organization, weather forecasting that  requires analysis of a \\nlarge amount of  data related to temperature, pressure  and humidity of various \\nplaces, etc.  \\n‚Ä¢ Accuracy : Computer provides a high degree of accuracy. For example, the  computer \\ncan accurately give the result of division of any two numbers up  to 10 decimal \\nplaces.  \\n‚Ä¢ Diligenc e: When used for a longer period of time, the computer does not  get tired or \\nfatigued. It can perform long and complex calculations with the  same speed and \\naccuracy from the start till the end.   \\n‚Ä¢ Storage Capability : Large volumes of data and information can be stored in  the \\ncomputer and also retrieved whenever required. A limited amount of  data can \\nbe stored, temporarily, in the primary memory. Secondary storage  devices like \\nfloppy disk and compact disk can store a l arge amount of data  permanently.  \\n‚Ä¢ Versatility : Computer is versatile in nature. It can perform different types of  tasks', metadata={'source': 'https://www.just.edu.jo/~mqais/CIS99/PDF/Ch.01_Introduction_%20to_computers.pdf', 'page': 8}),\n",
       " Document(page_content='9 The content of the human mind can be  classified into four categories:  \\n1. Data: symbols  \\n2. Information: data that are processed to be useful; provides answers to  \"who\", \"what\", \\n\"where\", and \"when\" questions  \\n3. Knowledge: application of data and information; answers \"how\"  questions  \\n4. Wisdom: evaluated understanding.  \\nWe need to understand that processing data produced Information and process  \\nInformation produces Knowledge and so on   \\nCharacteristics of Computer  \\nSpeed, accuracy, diligence, storage capability a nd versatility are some of the key  \\ncharacteristics of a computer. A brief overview of these characteristics are   \\n‚Ä¢ Speed : The computer can process data very fast, at the rate of millions of  instructions \\nper second. Some calculations that would have taken h ours and days to \\ncomplete otherwise, can be completed in a few seconds using  the computer. \\nFor example, calculation and generation of salary slips of  thousands of', metadata={'source': 'https://www.just.edu.jo/~mqais/CIS99/PDF/Ch.01_Introduction_%20to_computers.pdf', 'page': 8}),\n",
       " Document(page_content='7 Computers classification***   \\nComputers can be generally classified by size and power as follows, though there is  \\nConsiderable overlap:  \\n‚Ä¢ Personal computer : A small, single -user computer based on  a microprocessor. In \\naddition to the microprocessor, a personal computer has a keyboard for entering \\ndata, a monitor for displaying information, and a storage device for saving data.  \\n‚Ä¢ workstation : A powerful, single -user computer. A workstation is like  a personal \\ncomputer, but it has a more powerful microprocessor and a higher -quality \\nmonitor.  \\n‚Ä¢ minicomputer : A multi -user computer capable of supporting from 10 to hundreds of  \\nusers simultaneously.  \\n‚Ä¢ mainframe : A powerful multi -user computer capable of  supporting many hundreds \\nor thousands of users simultaneously.  \\n‚Ä¢ supercomputer : An extremely fast computer that can perform hundreds of millions \\nof instructions per second.  \\n \\nLaptop and Smartphone Computers', metadata={'source': 'https://www.just.edu.jo/~mqais/CIS99/PDF/Ch.01_Introduction_%20to_computers.pdf', 'page': 6}),\n",
       " Document(page_content='with the same ease. At one moment you can use the computer to  prepare a letter \\ndocument and in the next moment you may play music or  print a document.  \\nComputers have several limitations too. Computer can only perform tasks that \\nit has  been programmed to do.', metadata={'source': 'https://www.just.edu.jo/~mqais/CIS99/PDF/Ch.01_Introduction_%20to_computers.pdf', 'page': 8})]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## vector stores\n",
    "query = \"Characteristics of Computer\"\n",
    "db.similarity_search(query, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='1 Chapter One \\nIntroduction to Computer  \\n \\nComputer  \\nA computer is an electronic device, operating under the control of instructions stored \\nin its own memory that can accept data (input), process the data according to specified \\nrules, produce information (output), and store the information for future use1. \\n \\nFunctionalities of a computer2  \\nAny digital computer carries out five functions in gross terms:  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nComputer Components  \\nAny kind of computers consists of HARDWARE AND SOFTWARE . \\n \\nHardware:  \\nComputer hardware is the collection of  physical elements that constitutes a computer \\nsystem. Computer hardware refers to the physical parts or components of a computer \\nsuch as the monitor, mouse, keyboard, computer data storage, hard drive disk (HDD), \\nsystem unit (graphic cards, sound cards, m emory, motherboard and chips), etc. all of \\nwhich are physical objects that can be touched .3', metadata={'source': 'https://www.just.edu.jo/~mqais/CIS99/PDF/Ch.01_Introduction_%20to_computers.pdf', 'page': 0}),\n",
       " Document(page_content='employees of an organization, weather forecasting that  requires analysis of a \\nlarge amount of  data related to temperature, pressure  and humidity of various \\nplaces, etc.  \\n‚Ä¢ Accuracy : Computer provides a high degree of accuracy. For example, the  computer \\ncan accurately give the result of division of any two numbers up  to 10 decimal \\nplaces.  \\n‚Ä¢ Diligenc e: When used for a longer period of time, the computer does not  get tired or \\nfatigued. It can perform long and complex calculations with the  same speed and \\naccuracy from the start till the end.   \\n‚Ä¢ Storage Capability : Large volumes of data and information can be stored in  the \\ncomputer and also retrieved whenever required. A limited amount of  data can \\nbe stored, temporarily, in the primary memory. Secondary storage  devices like \\nfloppy disk and compact disk can store a l arge amount of data  permanently.  \\n‚Ä¢ Versatility : Computer is versatile in nature. It can perform different types of  tasks', metadata={'source': 'https://www.just.edu.jo/~mqais/CIS99/PDF/Ch.01_Introduction_%20to_computers.pdf', 'page': 8}),\n",
       " Document(page_content='9 The content of the human mind can be  classified into four categories:  \\n1. Data: symbols  \\n2. Information: data that are processed to be useful; provides answers to  \"who\", \"what\", \\n\"where\", and \"when\" questions  \\n3. Knowledge: application of data and information; answers \"how\"  questions  \\n4. Wisdom: evaluated understanding.  \\nWe need to understand that processing data produced Information and process  \\nInformation produces Knowledge and so on   \\nCharacteristics of Computer  \\nSpeed, accuracy, diligence, storage capability a nd versatility are some of the key  \\ncharacteristics of a computer. A brief overview of these characteristics are   \\n‚Ä¢ Speed : The computer can process data very fast, at the rate of millions of  instructions \\nper second. Some calculations that would have taken h ours and days to \\ncomplete otherwise, can be completed in a few seconds using  the computer. \\nFor example, calculation and generation of salary slips of  thousands of', metadata={'source': 'https://www.just.edu.jo/~mqais/CIS99/PDF/Ch.01_Introduction_%20to_computers.pdf', 'page': 8}),\n",
       " Document(page_content='7 Computers classification***   \\nComputers can be generally classified by size and power as follows, though there is  \\nConsiderable overlap:  \\n‚Ä¢ Personal computer : A small, single -user computer based on  a microprocessor. In \\naddition to the microprocessor, a personal computer has a keyboard for entering \\ndata, a monitor for displaying information, and a storage device for saving data.  \\n‚Ä¢ workstation : A powerful, single -user computer. A workstation is like  a personal \\ncomputer, but it has a more powerful microprocessor and a higher -quality \\nmonitor.  \\n‚Ä¢ minicomputer : A multi -user computer capable of supporting from 10 to hundreds of  \\nusers simultaneously.  \\n‚Ä¢ mainframe : A powerful multi -user computer capable of  supporting many hundreds \\nor thousands of users simultaneously.  \\n‚Ä¢ supercomputer : An extremely fast computer that can perform hundreds of millions \\nof instructions per second.  \\n \\nLaptop and Smartphone Computers', metadata={'source': 'https://www.just.edu.jo/~mqais/CIS99/PDF/Ch.01_Introduction_%20to_computers.pdf', 'page': 6}),\n",
       " Document(page_content='with the same ease. At one moment you can use the computer to  prepare a letter \\ndocument and in the next moment you may play music or  print a document.  \\nComputers have several limitations too. Computer can only perform tasks that \\nit has  been programmed to do.', metadata={'source': 'https://www.just.edu.jo/~mqais/CIS99/PDF/Ch.01_Introduction_%20to_computers.pdf', 'page': 8})]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## FAISS Vector Store\n",
    "from langchain_community.vectorstores import FAISS\n",
    "db = FAISS.from_documents(documents, OpenAIEmbeddings())\n",
    "query = \"Characteristics of Computer\"\n",
    "db.similarity_search(query, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
